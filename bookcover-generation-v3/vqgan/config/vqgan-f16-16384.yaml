data:
  image_files: ../../resources/kyobobook-images/**/*.jpg
  image_size: 384
  validation_ratio: 0.01

model:
  encoder:
    num_channels: 3
    num_layers: [2, 2, 2, 2, 2]
    hidden_dims: [32, 64, 128, 256, 256]
  decoder:
    num_channels: 3
    num_layers: [2, 2, 2, 2, 2]
    hidden_dims: [256, 256, 128, 64, 32]
  quantizer:
    num_embeddings: 16384
    embedding_dim: 256
    factorized_dim: 32
  discriminator:
    num_channels: 3
    base_dim: 64
  perceptual:
    architecture: efficientnetv2_s
    pretrained: clip-image-encoder.pth

optim:
  generator:
    lr: 5e-5
    betas: [0.5, 0.9]
    eps: 1e-6
  discriminator:
    lr: 5e-5
    betas: [0.5, 0.9]
    eps: 1e-6
  criterion:
    perceptual: 0.1
    quantization: 1
    adversarial: 0.75
  num_discriminator_steps: 1

train:
  name: vqgan-f16-16384
  epochs: 20
  batch_size: 32
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  validation_interval: 0.25
  log_every_n_steps: 10
