data:
  image_files: ../../resources/kyobobook-images/**/*.jpg
  image_size: 384
  validation_ratio: 0.01

model:
  encoder:
    num_channels: 3
    embedding_dim: 256
    num_layers: [2, 2, 4, 4, 4]
    hidden_dims: [128, 128, 256, 256, 512]
  decoder:
    num_channels: 3
    embedding_dim: 256
    num_layers: [4, 4, 4, 2, 2]
    hidden_dims: [512, 256, 256, 128, 128]
  quantizer:
    num_embeddings: 8192
    embedding_dim: 256
    factorized_dim: 32
    orthogonal_sampling: 256
  discriminator:
    num_channels: 3
    base_dim: 64
  perceptual:
    architecture: efficientnetv2_s
    pretrained: clip-image-encoder.pth

optim:
  generator:
    lr: 5e-5
    betas: [0.5, 0.9]
    eps: 1e-6
  discriminator:
    lr: 5e-5
    betas: [0.5, 0.9]
    eps: 1e-6
  criterion:
    perceptual: 0.1
    quantization: 1
    orthogonal: 10
    adversarial: 0.2
  num_discriminator_steps: 1

train:
  name: vqgan-256-8192
  epochs: 20
  batch_size: 32
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  validation_interval: 0.25
  log_every_n_steps: 10
